# hadoop helloworld

解压jdk, hadoop, 配置环境变量JAVA_HOME

```bash
sudo tar xzf jdk-8u151-linux-x64.tar.gz -C /usr/local/lib
sudo tar xzf hadoop-2.9.0.tar.gz -C ~/hadoop
sudo vim /etc/profile
export JAVA_HOME="/usr/local/lib/jdk1.8.0_151"
export CLASSPATH=".;$JAVA_HOME/lib/dt.jar;%JAVA_HOME%/lib/rt.jar;%JAVA_HOME%/lib/tools.jar"
export PATH="$PATH:$JAVA_HOME/bin"
export HADOOP_PREFIX="/home/li_wjie/hadoop/hadoop-2.9.0"
export HADOOP_CONF_DIR="$HADOOP_PREFIX/etc/hadoop"
export HADOOP_YARN_HOME="$HADOOP_PREFIX"
sudo vim /etc/hosts
192.168.56.120 master
192.168.56.121 hadoop1
192.168.56.122 hadoop2
192.168.56.123 hadoop3
```

进入hadoop解压目录

```bash
cd ~/hadoop/hadoop-2.9.0
```

查看hadoop命令说明

```bash
./bin/hadoop
```


独立模式

准备输入数据, 用于后续测试

```bash
mkdir input && cp ./etc/hadoop/*.xml input
```

从input读取文件, 查找满足正则表达式的字符串, 输出到output文件夹

```bash
./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.0.jar grep input output 'dfs[a-z.]+'
```


集群模式

编辑hadoop-env.sh, yarn-env.sh, slaves, core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml

```bash
vim ./etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/local/lib/jdk1.8.0_151
```

```bash
vim ./etc/hadoop/yarn-env.sh
export JAVA_HOME=/usr/local/lib/jdk1.8.0_151
```

```bash
vim ./etc/hadoop/slaves
hadoop1
hadoop2
```

```bash
vim ./etc/hadoop/core-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master:9000</value>
	</property>
</configuration>
```

```bash
vim ./etc/hadoop/hdfs-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>master:9001</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/hadoop/dfs/name</value>
	</property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
	<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>
</configuration>
```

```bash
vim ./etc/hadoop/mapred-site.xml
```

```xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	<property>
		<name>mapreduce.jobhistory.address</name>
		<value>master:10020</value>
	</property>
	<property>
		<name>mapreduce.jobhistory.webapp.address</name>
		<value>master:19888</value>
	</property>
</configuration>
```

```bash
vim ./etc/hadoop/yarn-site.xml
```

```yaml
<?xml version="1.0"?>
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>master:8032</value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>master:8030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>master:8035</value>
	</property>
	<property>
		<name>yarn.resourcemanager.admin.address</name>
		<value>master:8033</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>master:8088</value>
	</property>
</configuration>
```

检查ssh, 需要满足访问localhost无需输入密码

```bash
ssh localhost
sudo ssh localhost
```

如果需要输密码, 则

```bash
# 给自己生成rsa非对称算法的公钥和密钥
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
# 把自己的公钥, 加入authorized_keys, 这样以后登录ssh无需输入密码
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
# 修改authorized_keys文件权限, 只能自己读写
chmod 0600 ~/.ssh/authorized_keys
```

清理
sudo rm -r /hadoop
sudo rm $HADOOP_PREFIX/logs/*

格式化文件系统

```bash
sudo $HADOOP_PREFIX/bin/hdfs namenode -format
```

启动hdfs, NameNode后台服务和DataNode后台服务

```bash
sudo ./sbin/start-dfs.sh
sudo $HADOOP_PREFIX/sbin/start-dfs.sh
sudo $HADOOP_PREFIX/sbin/stop-dfs.sh
```

启动yarn

```bash
sudo ./sbin/start-yarn.sh
sudo $HADOOP_PREFIX/sbin/start-yarn.sh
sudo $HADOOP_PREFIX/sbin/stop-yarn.sh
```

查看集群状态

```bash
./bin/hdfs dfsadmin -report
```

查看hdfs, NameNode控制页面

```txt
http://hadoop0:50070/
```

查看RM

```txt
http://hadoop0:8088/


测试

touch data1 data2
for ((i=1;i<999999;i++))
do
echo “this is a test data1” >> data1
echo "and the data2 will be always created">>data2
done

sudo $HADOOP_PREFIX/bin/hadoop fs -mkdir /tmp
sudo $HADOOP_PREFIX/bin/hadoop fs -mkdir /tmp/input
sudo $HADOOP_PREFIX/bin/hadoop fs -put $HADOOP_PREFIX/data1 /tmp/input
sudo $HADOOP_PREFIX/bin/hadoop fs -put $HADOOP_PREFIX/data2 /tmp/input


